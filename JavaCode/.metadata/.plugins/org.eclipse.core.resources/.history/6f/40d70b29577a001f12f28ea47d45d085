package cs523.bitcoinprice.consumer;

import java.util.Arrays;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Set;

import kafka.serializer.StringDecoder;
import kafka.serializer.StringEncoder;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.function.VoidFunction;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.*;
import org.apache.spark.streaming.kafka.KafkaUtils;
import org.apache.spark.streaming.api.java.JavaPairInputDStream;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.log4j.Logger;
import org.apache.log4j.Level;

import scala.Tuple2;

import com.google.gson.Gson;
import com.google.gson.JsonArray;
import com.google.gson.JsonObject;
import com.google.gson.JsonParser;

public class BitcoinPriceListener {

    private static final Logger logger = Logger.getLogger(BitcoinPriceListener.class);

    public static void main(String[] args) throws Exception {

        // Set log level to WARN to reduce verbosity
        Logger.getLogger("org").setLevel(Level.WARN);

        // Configure Spark
        SparkConf conf = new SparkConf().setAppName("BitcoinPriceListener")
                .setMaster("local[*]");

        JavaStreamingContext sc = new JavaStreamingContext(conf, Durations.seconds(10));

        // Kafka parameters
        Map<String, String> kafkaParams = new HashMap<>();
  
        kafkaParams.put("metadata.broker.list", "localhost:9092");
        kafkaParams.put("group.id", "consumer-group");
        kafkaParams.put("auto.offset.reset", "largest");
        kafkaParams.put("bootstrap.servers", "localhost:9092");
        kafkaParams.put("key.deserializer", StringDeserializer.class.getName());
        kafkaParams.put("value.deserializer", StringDeserializer.class.getName());
        kafkaParams.put("enable.auto.commit", "false");
  

        Set<String> topics = new HashSet<>(Arrays.asList("bitcoin-price"));

        JavaPairInputDStream<String, String> stream = KafkaUtils.createDirectStream(
                sc,
                String.class,
                String.class,
                StringDecoder.class,
                StringDecoder.class,
                kafkaParams,
                topics
        );

        stream.foreachRDD(rdd -> {
            if (!rdd.isEmpty()) {

                rdd.foreach(record -> {
                    logger.info("Record Key: " + record._1() + ", Value: " + record._2());

                    try {
                        // Initialize Gson and JsonParser
                        Gson gson = new Gson();
                        JsonParser jsonParser = new JsonParser();

                        // Parse the record (assuming record._2() is a JSON string)
                        JsonObject jsonObject = jsonParser.parse(record._2()).getAsJsonObject();

                        // Access the data array inside the JSON object
                        JsonArray dataArray = jsonObject.getAsJsonArray("data");

                        // Loop through the data array and extract fields
                        for (int i = 0; i < dataArray.size(); i++) {
                            JsonObject tradeNode = dataArray.get(i).getAsJsonObject();
                            String assetId = tradeNode.get("assetId").getAsString();
                            double price = tradeNode.get("price").getAsDouble();
                            String timestamp = tradeNode.get("timestamp").getAsString();
                            double size = tradeNode.get("size").getAsDouble();

                            // Create a BitcoinPrice object
                            BitcoinPrice bitcoinPrice = new BitcoinPrice(assetId, price, timestamp, size);

                            // Populate data in HBase
                            BitcoinPriceHbaseTable.populateData(bitcoinPrice);
                        }

                    } catch (Exception e) {
                        logger.error("Error processing record: ", e);
                    }
                });
            }
        });

        // Start the streaming context and await termination
        sc.start();
        sc.awaitTermination();
    }
}
